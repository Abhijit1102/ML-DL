{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec9c068",
   "metadata": {},
   "source": [
    "In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. This is similar to the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities.\n",
    "\n",
    "Y = Activation function(∑ (weights*input + bias))\n",
    "\n",
    "They decide whether a neuron should be activated or not and it is a non-linear transformation that can be done on the input before sending it to the next layer of neurons or finalizing the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ab950",
   "metadata": {},
   "source": [
    "## Idea of activation function\n",
    "\n",
    "1. Capuring non linearity in the data\n",
    "2. Activation function must be differentiable but Relu is not differtentiable at zero\n",
    "3. computationaly inexpense\n",
    "4. zero centred or normalized e.g. tanh\n",
    "5. Non saturating: saturating mean schuuize e.g. sigmoid, tanh, non-saturating :relu, vanishing gradient descent problem occur saturation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c0433",
   "metadata": {},
   "source": [
    "### sigmoid\n",
    "\n",
    "<img src='https://miro.medium.com/max/1100/1*I2tGxczJXJkN_P2Bpu9SzQ.webp'>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1100/1*dKz7wC7Qy17NoG2GVbWvAQ.webp\">\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1100/1*6A3A_rt4YmumHusvTvVTxw.webp\">\n",
    "\n",
    "## Advatages of Sigmoid\n",
    "\n",
    "- can be used in binary classaification due to prbabilistic in nature\n",
    "- non linear function\n",
    "- its diffrentiable\n",
    "\n",
    "## Disadvatages\n",
    "- its saturating function resulting in vanishing gradient problem\n",
    "- non zero cerntred \n",
    "- invoves exponential\n",
    "## mostly sigmoid is used ouput layers for binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0a4ac",
   "metadata": {},
   "source": [
    "## tan-hyperbolic\n",
    "- The hyperbolic tangent activation function is also referred to simply as the Tanh (also “tanh” and “TanH“) function. It is very similar to the sigmoid activation function and even has the same S-shape. The function takes any real value as input and outputs values in the range -1 to 1.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/720/1*ZafDv3VUm60Eh10OeJu1vw.webp\">\n",
    "\n",
    "## Adavatage\n",
    "- Non linear\n",
    "- Differentable\n",
    "- zero centred\n",
    "- training is faster\n",
    "\n",
    "## Disadvartages\n",
    "- Saturating function\n",
    "- computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1407e03c",
   "metadata": {},
   "source": [
    "## relu function (rectified linear unit)\n",
    "ReLU formula is : f(x) = max(0,x)\n",
    "\n",
    "## advatage\n",
    "- relu is non linear function\n",
    "- not saturated in the positive region\n",
    "- computationaly in expensive\n",
    "\n",
    "## Disadvatage\n",
    "\n",
    "- not differentiable at zero\n",
    "- not zero centred\n",
    "  ->  to over com it we used batch normalization\n",
    "\n",
    "##  Dying relu problem\n",
    "- A dead ReLU pretty much just means that its argument value is negative such that the gradient stays at 0; no matter how you train it from that point on. You can simply have a look at the gradient during training to see whether a ReLU is dead or not.\n",
    "\n",
    "- i.e. some times ouput of neuron becomes always zero when that input is given is called death neurons, if 50% of neuron is death we cant caputure of representation of data. \n",
    "\n",
    "- Leaky ReLU is a common effective method to solve a dying ReLU problem, and it does so by adding a slight slope in the negative range. This modifies the function to generate small negative outputs when input is less than 0\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f6e99b",
   "metadata": {},
   "source": [
    "## Solution of dying relu problem\n",
    "- set low  learning rate\n",
    "- bais -> postive value\n",
    "-  used other variant of relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a6eae",
   "metadata": {},
   "source": [
    "## Type of Relu - variant\n",
    "- linear(Leaky-Relu, Parametric-Relu)\n",
    "- Non linear(Elu[exponential linear unit], SELU(The Scaled Exponential Linear Unit ) \n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/319438080/figure/fig1/AS:960500053602304@1606012466585/shape-of-ReLU-and-its-variants.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9df6f3",
   "metadata": {},
   "source": [
    "Leaky Relu Advantage\n",
    "  f(x) = x if x>0\n",
    "         0.1x other\n",
    "- Non saturated, unboundrd\n",
    "- easilty computed\n",
    "- No dying relu problem\n",
    "- close to 0 centered\n",
    "\n",
    "parametric relu\n",
    " f(x) = x if x>0\n",
    "        ax otherwise\n",
    "        \n",
    "ELU = x if x>0\n",
    "      a(e^x -1) if x<0\n",
    "   - its always differentiable\n",
    "   - value is closed to zero centred so convergence of solution is faster\n",
    "   - always continiuos and differentiable\n",
    "   \n",
    "Disadvantage is computatiopnally expensive\n",
    "\n",
    "SELU = lamda (ElU)\n",
    "      : lamda and a are need to given once\n",
    "\n",
    " Its advantage is self normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bce554f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0fe968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n):\n",
    "    if n== 1 or n==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return fib(n-1) + fib(n-2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "990e9ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165580141\n",
      "72.23208618164062\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "print(fib(40))\n",
    "print(time.time() -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdca3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "d ={0:1,1:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3a75ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of dynamic programming\n",
    "# fib\n",
    "\n",
    "def fib(n,d):\n",
    "    if n in d:\n",
    "        return d[n]\n",
    "    else:\n",
    "        d[n]=fib(n-1,d) + fib(n-2,d)\n",
    "        return d[n]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f40fdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165580141\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(fib(40,d))\n",
    "print(time.time() -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256a011d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f57123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
